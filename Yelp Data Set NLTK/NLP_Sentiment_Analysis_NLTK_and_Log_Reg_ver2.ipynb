{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Sentiment Analysis - NLTK and Log Reg ver2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MsFiGPv78PgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Sentiment analyzer with nltk for dealing with tokenization of words and \n",
        "#Logistic Regression for classification; Other techniques/libraries include\n",
        "#NLTK and Beautiful Soup.\n",
        "#Uses Yelp reviews for input data.\n",
        "#\n",
        "#Based on LazyProgrammer's Sentiment Analysis @ \n",
        "#https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class/sentiment.py\n",
        "#Stopwords file from:\n",
        "#http://www.lextek.com/manuals/onix/stopwords1.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FW1g731a8mD4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import all the libraries\n",
        "import nltk\n",
        "#nltk.download(\"popular\") #for Google Colab\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from future.utils import iteritems\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from bs4 import BeautifulSoup #to build the word tree as a nested data structure\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xX6ziXF98n2H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#this line is only necessary if you are using colaboratory\n",
        "#data = files.upload()\n",
        "#delete files uploaded with colab:\n",
        "#!rm negative.review\n",
        "#!rm positive.review\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxEek0Dz8rIQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Lemmatizer to root words to basic forms\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_7Phsr2T81B_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#stop words are articles and other components of speech that don't really add context\n",
        "#want to remove the stopwords\n",
        "stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwfQppH383Sg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Use BeautifulSoup to work through the word trees\n",
        "positive_reviews = BeautifulSoup(open('positive.review').read())\n",
        "positive_reviews = positive_reviews.findAll('review_text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iP5jUzXH85Df",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Use BeautifulSoup to work through the word trees\n",
        "negative_reviews = BeautifulSoup(open('negative.review').read())\n",
        "negative_reviews = negative_reviews.findAll('review_text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zdIit-Ra861A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#see how big these soups are\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B3tjAFqT899X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fix the positive_review soup to be the same length as the negative, \n",
        "#but randomize entries first\n",
        "np.random.shuffle(positive_reviews)\n",
        "positive_reviews = positive_reviews[:len(negative_reviews)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5OVWj-O9DIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "def my_tokenizer(s):\n",
        "    s = s.lower() #consider lowercase versions of words\n",
        "    tokens = nltk.tokenize.word_tokenize(s) #use NLTKs tokenizer\n",
        "    tokens = [t for t in tokens if len(t) > 2] #get rid of short words\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] #lemmatize the current word\n",
        "    tokens = [t for t in tokens if t not in stopwords] # cut the stopwords\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUtnt4yi9Fzf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#count up the new words, when observed\n",
        "word_index_map = {} #make an empty array\n",
        "current_index = 0 #start at position 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XTxKHZCq-V3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#backup our tokenized arrays\n",
        "positive_tokenized = []\n",
        "negative_tokenized = []\n",
        "orig_reviews = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuPaTciA-bL_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for review in positive_reviews:\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    positive_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "for review in negative_reviews:\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    negative_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWQH4jkG-lfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#take each token and build numerical array\n",
        "def tokens_to_vector(tokens, label):\n",
        "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
        "    for t in tokens:\n",
        "        i = word_index_map[t]\n",
        "        x[i] += 1\n",
        "    x = x / x.sum() # normalize it before setting label\n",
        "    x[-1] = label\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MeOVkKdD-ym4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = len(positive_tokenized) + len(negative_tokenized)\n",
        "\n",
        "#assign zero array\n",
        "data = np.zeros((N, len(word_index_map) + 1))\n",
        "\n",
        "#counter to track example #\n",
        "i = 0\n",
        "\n",
        "#now loop through the positives\n",
        "for tokens in positive_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 1)\n",
        "    data[i,:] = xy\n",
        "    i += 1\n",
        "\n",
        "#now loop through the negatives\n",
        "for tokens in negative_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 0)\n",
        "    data[i,:] = xy\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJvmwgTA-_t_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#shuffle everything again\n",
        "np.random.shuffle(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "10RXevII_Dk_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = data[:, :-1] #all the rows except last column\n",
        "Y = data[:, -1] #just the last column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7utqJl3T_FVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#split sets at position -100, the last 100 columns\n",
        "Xtrain = X[:-100,]\n",
        "Ytrain = Y[:-100,]\n",
        "Xtest = X[-100:,]\n",
        "Ytest = Y[-100:,]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8SuK7BNP_Gvn",
        "colab_type": "code",
        "outputId": "9b2fee3e-87b2-4a52-a680-e918ad69dc38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "#build the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "#fit the model to the training data\n",
        "model.fit(Xtrain, Ytrain)\n",
        "\n",
        "#print the results!\n",
        "print(\"Classification rate:\", model.score(Xtest, Ytest))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Classification rate: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "egBE54FF_Il_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#classification accuracy of 73% match rate\n",
        "#maybe logistic regression is not the best way to go with this classification task\n",
        "#could try linear regression or a binary classification method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pfPa8noEGR8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "1d5ccb60-2ad6-49b1-caef-675230090e80"
      },
      "cell_type": "code",
      "source": [
        "#so now going the threshold this to see what words are outside the threshold range\n",
        "threshold = 0.5\n",
        "for word, index in iteritems(word_index_map):\n",
        "  weight = model.coef_[0][index]\n",
        "  if weight > threshold or weight < -threshold:\n",
        "    print (word, weight)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "great 2.5522660430882635\n",
            "for 1.9855946653860324\n",
            "the -0.8599716927582726\n",
            "you 0.8217295415341865\n",
            "get -0.7471046075893204\n",
            "back -1.0665135987144818\n",
            "and 1.3308624250814878\n",
            "price 1.6422856336829266\n",
            "wa -0.9589979474814412\n",
            "with 1.083209277692315\n",
            "will -0.6813811815821088\n",
            "memory 0.5864286073937015\n",
            "they -0.6900979322823695\n",
            "but -0.5922336796571961\n",
            "perfect 0.5710921849022103\n",
            "good 1.441334510170496\n",
            "not -3.164017889934406\n",
            "that -0.5724662221870235\n",
            "sound 0.7803251216644143\n",
            "excellent 0.8604058721658995\n",
            "well 0.6605882127174212\n",
            "used 0.6533468340810541\n",
            "doe -0.7033137039239062\n",
            "thing -0.5985298054189825\n",
            "highly 0.5969834638643131\n",
            "use 1.067658764804554\n",
            "time -0.5108544663048894\n",
            "out -0.8101020971103214\n",
            "did -0.6049648478617873\n",
            "n't -1.4331653139847305\n",
            "are 1.0683896979642196\n",
            "best 0.6725147513938603\n",
            "very 1.023435954020525\n",
            "buy -0.6402108370315929\n",
            "then -0.6540882844032528\n",
            "quality 0.983810343395668\n",
            "after -1.078835384042973\n",
            "cable 0.596328946973414\n",
            "speaker 0.544350808841778\n",
            "fast 0.5076075129105997\n",
            "easy 0.9444163555076407\n",
            "item -0.5840299929571096\n",
            "support -0.5332390902353161\n",
            "money -0.6475658186890597\n",
            "love 0.6160718614948493\n",
            "return -0.6783553327964735\n",
            "waste -0.607979220210822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DYsY-CxDGtAH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
